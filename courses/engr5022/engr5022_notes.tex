%Algebra Notes
%By Salvatore Giorgi
%version 1.0
%This is the first version. 
%\documentclass[draftclsnofoot,a4paper,12pt,onecolumn,doublespace,compsoc]{IEEEtran}
\documentclass[conference,12pt,onecolumn,compsoc]{IEEEtran}

\usepackage{array}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage[cmex10]{amsmath}
\usepackage{array}
\usepackage{fixltx2e}
\usepackage{cite}
\usepackage[center]{caption}
\usepackage{times}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage{caption}
\usepackage{float}
\usepackage{epstopdf}
\usepackage{url}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{amssymb}
\usepackage{stfloats}
\usepackage{float}
%\usepackage[tight,footnotesize]{subfigure}
%\usepackage[caption=false]{caption}
\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{subfigure}
\usepackage{subfig}
%%%%%%%%%%%%%%%tikz package%%%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{verbatim}
\usepackage{xcolor}
\usetikzlibrary{shapes}
\usetikzlibrary{decorations.markings}
\usepackage{rotating}



\title{\Large ENGR 5022 Engineering Analysis\\
\normalsize Course Notes \\
\normalsize March 23, 2013}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Salvatore Giorgi}
\IEEEauthorblockA{salvatore.giorgi@temple.edu}
}



%%%%%%%%%%%%%%%%%%% DOCUMENT START %%%%%%%%%%%%%%%%

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]

%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
These notes were taken during the 2012-2013 academic year for Temple University's ENGR 5022 Engineering Analysis course. The outline follows the given course notes as well as the following additional sources: "Fourier Series and Orthogonal Functions" by Harry F. Davis, "Linear Algebra Done Right" by Sheldon Axler, and "Advanced Linear Algebra" by Steven Roman. Propositions and theorems are supplied by the source material while the proofs were written by me with the aid of the source material only as a last result. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Linear Spaces %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Spaces}
\label{section: Linear Spaces}

\begin{proposition}
Every finite-dimensional inner-product space has an orthonormal basis.
\end{proposition}
\begin{proof}

\end{proof}

\begin{proposition}
Every orthonormal list of vectors in $V$ can be extended to an orthonormal basis in $V$.
\end{proposition}
\begin{proof}

\end{proof}

\begin{proposition}
Suppose $T\in \mathcal{L}(V)$. If $T$ has an upper-triangular matrix with respect to some basis of $V$ then $T$ has an upper triangular matrix with respect to some orthonormal basis of $V$
\end{proposition}
\begin{proof}

\end{proof}

\begin{proposition}
Schur's Theorem: Suppose $V$ is a complex vector space and $T\in \mathcal{L}(V)$. Then $T$ has an upper triangular matrix with respect to some orthonormal basis of $V$
\end{proposition}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Orthogonal Projections and Minimization Problems}
\label{section: Orthogonal Projections and Minimization Problems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear Functionals and Adjoints}
\label{section: Linear Functionals and Adjoints}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Orthogonal Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Orthogonal Functions}
\label{section: Orthogonal Functions}

The central idea of this section is the following. We are given a sequence of functions $f_1(x), f_2(x), ...$ which have the following property:
\begin{equation}
\int_a^b f_i(x)\overline{f_j(x)}dx = 0
\nonumber
\end{equation}
for all $i\neq j$. We desire to expand an arbitrary function $F(x)$ in an infinite series
\begin{equation}
F(x) = c_1f_1(x)+ c_2f_2(x)+ \cdots.
\nonumber
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inner Products}
\label{section: Inner Products}

\begin{definition}
An \emph{inner product} of a linear space is a function of pairs of elements of the space, that is, to each pair $f$ and $g$ in the linear space there is a number $(f|g)$ satisfying the following axioms:
\begin{enumerate}[(1)]
	\item $\langle f|f\rangle \geq 0$ for every $f$ (Positivity)
	\item $\langle f|f\rangle = 0 \Leftrightarrow f=0$ (Definiteness)
	\item $\langle f|g\rangle = \overline{\langle g|f\rangle}$ for every pair $f$ and $g$ (Conjugate Symmetry)
	\item $\langle\alpha f + \beta g | h\rangle = \alpha\langle f|h\rangle + \beta\langle g|h\rangle$ (Additivity and Homogeneity in the First Slot)
\end{enumerate}
where $\alpha$ and $\beta$ are arbitrary numbers. 
\end{definition}

\begin{definition}
An \emph{inner product space} is a vector space $V$ along with an inner product on $V$. 
\end{definition}

\begin{definition}
The \emph{norm} is defined as
\begin{equation}
||f|| = \langle f|f\rangle^{\frac{1}{2}}
\label{norm}
\end{equation}
\end{definition}

\begin{proposition}
Properties of the norm:
\begin{enumerate}[(1)]
	\item $||f|| \geq 0$
	\item $||f|| = 0 \Leftrightarrow f = 0$
	\item $||\alpha f|| = |\alpha| ||f||$
	\item $||f+g|| \leq ||f|| + ||g||$
\end{enumerate}
\end{proposition}
\begin{proof}

\end{proof}

\begin{proposition}
\emph{Cauchy - Schwarz Inequality}:
\begin{equation}
|\langle f,g\rangle|\leq ||f|| ||g||
\label{schwarz}
\end{equation}
\end{proposition}
\begin{proof}

\end{proof}

\begin{proposition}
\emph{Minkowski Inequality}:
\begin{equation}
||f+g|| \leq ||f|| + ||g||
\label{Minkowski}
\end{equation}
\end{proposition}
\begin{proof}

\end{proof}

\begin{proposition}
\emph{Triangle Inequality}:
\begin{equation}
||f-g|| \leq ||f-h|| + ||h-g||
\label{triangle}
\end{equation}
\end{proposition}
\begin{proof}

\end{proof}

If we drop the requirement that $\langle f,g\rangle = 0$ only when $f = 0$ then we call $\langle f,g\rangle$ a \emph{pseudo-inner product} and $||f||$ a \emph{pseudo-norm}. 

Examples of inner products:
\begin{enumerate}[(1)]
	\item In $\mathbb{R}^n$ we have $\langle x,y\rangle = x_1y_1 + x_2y_2 + ... + x_ny_n$
	\item In $\mathbb{C}^n$ we have $\langle x,y\rangle = x_1\bar{y}_1 + x_2\bar{y}_2 + ... + x_n\bar{y}_n$
	\item In the space of all continuous functions of period $2\pi$ we have \begin{equation}
	\langle f,g\rangle = \int_0^{2\pi} f(x)\overline{g(x)}dx
	\nonumber
	\end{equation}
	\item Let $f$ and $g$ be bounded, integrable functions of $x$, and let $r$ be a fixed function, defined and continuous in the interval $a\leq x \leq b$, having the property $r(x)>0$ whenever $a<x<b$. Then the inner product with respect to the weight function $r$ is defined as
	\begin{equation}
	\langle f,g\rangle = \int_a^b f(x)\overline{g(x)}r(x)dx
	\nonumber
	\end{equation}
\end{enumerate}

\begin{definition}
Two elements $f$ and $g$ of a vector space are called \emph{orthogonal} if $\langle f,g\rangle$.
\end{definition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Orthogonal Sequences}
\label{section: Orthogonal Sequences}

In this section $V$ is an inner product space.

\begin{definition}
Let $\phi$ be an element of $V$  having unit norm. For an $f$ of class $V$ the \emph{projection of $f$ in the direction of $\phi$} is denoted proj($f|\phi$) and is defined by
\begin{center}
proj($f:\phi)=\langle f, \phi\rangle\phi$
\end{center}
\end{definition}

\begin{theorem}
The projection of $f$ in the direction of $\phi$ is equal to $f$ $\Leftrightarrow$ $f$ is a scalar multiple of $\phi$
\end{theorem}
\begin{proof}
If proj($f:\phi$) = $f$ then by definition we have $f=\langle f, \phi\rangle\phi$, i.e. a scalar multiple of $\phi$. If $f=\alpha \phi$ then proj($f:\phi)=\langle f, \phi\rangle\phi = \langle \alpha \phi, \phi\rangle\phi  = \alpha \langle \phi, \phi\rangle\phi = \alpha\phi = f$. 
\end{proof}

\begin{theorem}
$f$-proj($f:\phi$) is orthonormal to $\phi$
\end{theorem}
\begin{proof}

\end{proof}

Let us assume we are given $n$ mutually orthogonal elements of $V$, all having unit norm: $\phi_1, \cdots, \phi_n$, where $\langle \phi_i, \phi_j\rangle = 0$ for all $i\neq j$ and $||\phi_j|| = 1$ for all $j$. These $n$ elements of $V$ span a subspace of $V$ which we denote $H_n$. 

\begin{definition}
The projection of $f$ into the subspace $H_n$ is defined by
\begin{equation}
proj(f:\phi_1, \phi_2, ..., \phi_n) = \langle f, \phi_1\rangle \phi_1+ ... + \langle f, \phi_n\rangle \phi_n
\label{subspace projection}
\end{equation}
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Differential Operators}
\label{section: Differential Operators}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Intergral Operators}
\label{section: Intergral Operators}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convolution and Dirichlet Kernel}
\label{section: Convolution and Dirichlet Kernel}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Fourier Series %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fourier Series}
\label{section: Fourier Series}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definitions and Examples}
\label{section: Definitions and Examples}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Local Convergence of Fourier Series}
\label{section: Local Convergence of Fourier Series}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uniform Convergence}
\label{section: Uniform Convergence}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence of Fourier Series}
\label{section: Convergence of Fourier Series}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Divergent Series}
\label{section: Divergent Series}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalized Functions}
\label{section: Generalized Functions}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Legendre Polynomials and Bessel Functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Legendre Polynomials and Bessel Functions}
\label{section: Legendre Polynomials and Bessel Functions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Partial Differential Equations}
\label{section: Partial Differential Equations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Laplacian Operator}
\label{section: The Laplacian Operator}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Legendre Polynomials}
\label{section: Legendre Polynomials}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Laplace's Equation in Spherical Coordinates}
\label{section: Laplace's Equation in Spherical Coordinates}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Spherical Harmonics}
\label{section: Spherical Harmonics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bessel Functions}
\label{section: Bessel Functions}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% The Fourier Integral %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Fourier Integral}
\label{section: The Fourier Integral}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Algebraic Concepts in Analysis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algebraic Concepts in Analysis}
\label{section: Algebraic Concepts in Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Functions on Groups %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Functions on Groups}
\label{section: Functions on Groups}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% The Eigenvalue Problem %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Eigenvalue Problem}
\label{section: The Eigenvalue Problem}

Given a square matrix \textbf{A} of size $n \times n$, if there exists a scalar $\lambda$ and a non trival $n$-vector \textbf{v} such that
\begin{equation}
\textbf{Av}=\lambda\textbf{v}
\label{eigenvalue equation 1}
\end{equation}

then $\lambda$ is called an \emph{eigenvalue} of the matrix \textbf{A} and \textbf{v} is called an \emph{eigenvector} of \textbf{A} corresponding to the eigenvalue $\lambda$. We note that from the above, the vector \textbf{v} is invariant under the transformation \textbf{A}. In view of this equation, for $\lambda$ to be an eigenvalue, it is necessary that
\begin{equation}
(\textbf{A} - \lambda\textbf{I})\textbf{v} = \textbf{0}
\label{eigenvalue equation 2}
\end{equation}
be satisfied for a nonzero vector \textbf{v}. Equation \eqref{eigenvalue equation 2} has a nonzero solution if and only if the determinant of the coefficient matrix is zero, i.e.,
\begin{equation}
det(\textbf{A} - \lambda\textbf{I})= 0.
\label{eigenvalue determinant equation}
\end{equation}\eqref{eigenvalue equation 2} also shows that the vector \textbf{v} must belong to the null space of the matrix $(\textbf{A}-\lambda\textbf{I})$ and therefore the eigenvectors can be computed by finding the null space of $(\textbf{A}-\lambda\textbf{I})$. 

We note that a matrix can have a nonempty null space only if it is singular, i.e. its determinant is zero. Equation \eqref{eigenvalue determinant equation} can be expanded as follows
\begin{equation}
|\textbf{A}-\lambda\textbf{I}| = (-1)^n\left[ \lambda^n+c_{n-1}\lambda^{n-1} + ... + c_1\lambda + c_0\right[ = 0,
\label{characteristic equation}
\end{equation}
where the the coefficients $c_0, c_1, ..., c_{n-1}$ are computed from the matrix. We see that \eqref{characteristic equation} is a polynomial of degree $n$ and thus has $n$ solutions. Thus, we have $n$ eigenvalues of the matrix \textbf{A}. Also, corresponding to each eigenvalue, there exists an element in the null space of $(\textbf{A}-\lambda\textbf{I})$. Therefore, we have, for an $n \times n$ matrix, exactly $n$ eigenvalues but not necessarily a full set of $n$ eigenvectors. 

Since any multiple of an element in the null space is also an element of the null space, we have that \emph{any multiple of an eigenvector is also an eigenvector}. This can be easily seen from \eqref{eigenvalue equation 1}.

For matrices with real entries, \emph{complex eigenvalues always appear in conjugate pairs}. SImilarly, the eigenvectors corresponding to complex eigenvalues (of real matrices) will always occur in conjugate pairs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Properties of Eigenvalues and Eigenvectors}
\label{section: Properties of Eigenvalues and Eigenvectors}

\begin{proposition}
Proof by induction. Base Case ($k=2)$: If $\lambda$ is an eigenvalue of a matrix \textbf{A}, then $\lambda^k$ is an eigenvalue of the matrix $\textbf{A}^k$, where $k$ is any integer positive or negative.
\end{proposition}
\begin{proof}
First consider $k\in\mathbb{Z}^+$. By definition we have $\lambda$ satisfying 
\begin{center}
$\textbf{Av} = \lambda\textbf{v}$.
\end{center} 
Multiplying the above by \textbf{A} results in
\begin{center}
$\textbf{A}^2\textbf{v}=\lambda\textbf{Av}=\lambda^2\textbf{v}$,
\end{center}
which shows that $\lambda^2$ is an eigenvalue of $\textbf{A}^2$.

Now assume the statement holds for $k = n$. Thus we have
\begin{center}
$\textbf{A}^n\textbf{v}=\lambda^n\textbf{v}$.
\end{center}
Again, multiply both sides by \textbf{A}, which results in
\begin{center}
$\textbf{A}^{n+1}\textbf{v}=\textbf{A}\textbf{A}^{n}\textbf{v}=
\textbf{A}\lambda^n\textbf{v}=\lambda^n\textbf{A}\textbf{v}=\lambda^{n+1}\textbf{v}$.
\end{center}

The same sets can show the result holds for $k<0$.
\end{proof}

\begin{proposition}
If the characteristic equation of a matrix \textbf{A} is given by \eqref{characteristic equation} then the determinant of the matrix is
\begin{equation}
det(\textbf{A}) = (-1)^nc_0
\label{determinant equation}
\end{equation}
\end{proposition}
\begin{proof}
Set $\lambda=0$ in \eqref{characteristic equation} and the result follows.
\end{proof}

\begin{proposition}
The matrix \textbf{A} is invertible if and only if $\lambda=0$ is not an eigenvalue of the matrix.
\end{proposition}
\begin{proof}
If \textbf{A} is not invertible then det(\textbf{A})=0. Thus by \eqref{determinant equation} we have $c_0=0$. Setting $c_0=0$ in \eqref{characteristic equation} we see that the characteristic equation equals zero when $\lambda=0$. 
\end{proof}

\begin{proposition}
The eigenvectors corresponding to the distinct eigenvalues are linearly independent.
\end{proposition}
\begin{proof}
We prove this by contradiction. Let $lamba_i$, for $i=1,...,n$ be distinct eigenvalues of a matrix \textbf{A} with the corresponding eigenvectors $\textbf{v}_i$. Assume that the eigenvectors $\textbf{v}_1, ..., \textbf{v}_k$, with $k<n$, are linearly independent. Thus, $\textbf{v}_{k+1}, ..., \textbf{v}_n$ are linearly dependent. Therefore, the following equation holds
\begin{equation}
c_1\textbf{v}_1+c_2\textbf{v}_2+ ... + c_k\textbf{v}_k + c_{k+1}\textbf{v}_{k+1} = 0
\label{proposition equation 1}
\end{equation}
for some (not all zero) constants $c_1, ..., c_{k+1}$. Next we multiply the above by \textbf{A} and use the fact that
\begin{equation}
\textbf{A}\textbf{v}_1 = \lambda_1\textbf{v}_1, ..., \textbf{A}\textbf{v}_{k+1} = \lambda_{k+1}\textbf{v}_{k+1}.
\nonumber
\end{equation}
This results in
\begin{equation}
c_1\lambda_1\textbf{v}_1 + ... + c_{k+1}\lambda_{k+1}\textbf{v}_{k+1} = 0. 
\label{proposition equation 2}
\end{equation}
Finally, we multiply \eqref{proposition equation 1} by $\lambda_{k+1}$ and subtract from \eqref{proposition equation 2} which results in
\begin{equation}
c_1(\lambda_1 - \lambda){k+1})\textbf{v}_1 + c_2(\lambda_2 - \lambda){k+1})\textbf{v}_2 + ... + c_k(\lambda_k - \lambda){k+1})\textbf{v}_k = 0.
\label{proposition equation 3}
\end{equation}
By assumption we have the eigenvectors $\textbf{v}_1, ..., \textbf{v}_k$ are linearly independent and thus the coefficients in \eqref{proposition equation 3} must all be equal to zero. Since each eigenvalue is distinct it follows that the coefficients $c_1 = c_2 = ... = c_k = 0$. From \eqref{proposition equation 2} it follows that
\begin{center}
$c_{k+1}\textbf{v}_{k+1} = 0$.
\end{center}
With $\textbf{v}_{k+1}$ an eigenvector, it is nonzero, and thus $c_{k+1}=0$, which is a contradiction since we assume the coefficients $c_1, ..., c_k, c_{k+1}$ are not all zero. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Similarity Transformation}
\label{section: Similarity Transformation}

\begin{definition}
A matrix \textbf{B} is said to be \emph{similar} to the matrix \textbf{A} if there exists a nonsingular transformation matrix \textbf{T} such that
\begin{equation}
\textbf{B} = \textbf{T}^{-1}\textbf{A}\textbf{T}
\label{transformation matrix}
\end{equation}
and the matrix \textbf{T} is called the similarity transformation
\end{definition}

\begin{proposition}
The eigenvalues of a matrix are invariant under similarity transformation, i.e., if \textbf{A} and \textbf{B} are similar matrices then they have the same eigenvalues. 
\end{proposition}
\begin{proof}
We use the characteristic equation for \textbf{B}:
\begin{center}
$0 = |\textbf{B} - \lambda_i\textbf{I}| $ \\
$= |\textbf{T}^{-1}\textbf{A}\textbf{T} - \lambda_i\textbf{I}| $ \\
$= |\textbf{T}^{-1}\textbf{A}\textbf{T} - \lambda_i\textbf{T}^{-1}\textbf{T}| $ \\
$= |\textbf{T}^{-1}(\textbf{A}-\lambda_i\textbf{I})\textbf{T}| $ \\
$= |\textbf{T}^{-1}||\textbf{A}-\lambda_i\textbf{I}||\textbf{T}| $ \\
$= |\textbf{A}-\lambda_i\textbf{I}| $ \\
\end{center}
since $|\textbf{T}^{-1}| = \frac{1}{|\textbf{T}|}$
\end{proof}

Next we find the relation between the eigenvectors of similar matrices. Supposed $\textbf{v}_i$ is an eigenvector of \textbf{B} corresponding to the eigenvalue $\lambda_i$. Thus, we have
\begin{center}
$\textbf{Bv}_i = \lambda_i\textbf{v}_i$.
\end{center}
Using $\textbf{B} = \textbf{T}^{-1}\textbf{AT}$ results in 
\begin{center}
$\textbf{T}^{-1}\textbf{AT}v_i = \lambda_i\textbf{v}_i$.
\end{center}
We premultiply both sides of the above equation by \textbf{T}
\begin{center}
$\textbf{ATv}_i = \lambda_i\textbf{Tv}_i$.
\end{center}
Denote $\textbf{u}_i = \textbf{Tv}_i$, which results in
\begin{center}
$\textbf{Au}_i = \lambda_i\textbf{u}_i$,
\end{center}
which says that $\textbf{u}_i = \textbf{Tu}_i$ is an eigenvector of \textbf{A}.

The choice of a similarity transformation for a given system is not arbitrary. The diagonal matrix is a useful form of similar matrix, which is introduced in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diagonalization of Matrices}
\label{section: Diagonalization of Matrices}

Given an arbitrary matrix \textbf{A} we would like to find a similarity transformation \textbf{T} such that
\begin{equation}
\textbf{T}^{-1}\textbf{AT} = \textbf{D}
\label{diagonal transformation}
\end{equation}
where \textbf{D} is a diagonal matrix. If there exists a transformation \textbf{T} for which \eqref{diagonal transformation} holds, we say that the matrix \textbf{A} is diagonalizable. 

\begin{proposition}
Suppose the eigenvalues of the n $\times$ n matrix \textbf{A} are all distinct. Then \textbf{A} is diagonalizable using the transformation matrix
\begin{equation}
\textbf{T} = [ \textbf{v}_1 \textbf{v}_2 \cdots \textbf{v}_n ],
\label{diagonal transformation matrix}
\end{equation}
where $\lbrace v_1, v_2, ..., v_n\rbrace$ are the eigenvectors of \textbf{A}. Futhermore, the diagonal entries of the matrix $\textbf{D} = \textbf{T}^{-1}\textbf{AT}$ are the eigenvalues of the matrix \textbf{A}.
\end{proposition}
\begin{proof}
First, we note that \textbf{T} is a nonsingular matrix since the eigenvectors are linearly independent. Nextm let the eigenvalues of the matrix \textbf{A} be $\lbrace \lambda_1, ..., \lambda_n\rbrace$, which are all distinct, and let $\lbrace \textbf{v}_1, ..., \textbf{v}_n\rbrace$ be the corresponding eigenvectors satisfying $\textbf{Av}_i = \lambda_i$, for $i=1, ..., n$. Then we have
\begin{center}
$\textbf{AT} = \textbf{A} [ \textbf{v}_1 \textbf{v}_2 \cdots \textbf{v}_n ]$ \\
$ =  [ \textbf{Av}_1 \textbf{Av}_2 \cdots \textbf{Av}_n ]$ \\
$ =  [ \lambda_1\textbf{v}_1 \lambda_2\textbf{v}_2 \cdots \lambda_n\textbf{v}_n ]$ \\
$ =  [ \textbf{v}_1 \textbf{v}_2 \cdots \textbf{v}_n ]\begin{bmatrix} \lambda_1 & 0 & 0 & ... & 0 \\ 
0 & \lambda_2 & 0 & ...  & 0 \\

\vdots & \vdots & \vdots & \ddots  & \vdots \\
0 & 0 & 0 & ...  & \lambda_n   \end{bmatrix}$ \\

$ =  \textbf{TD}$ .
\end{center}
Thus we have $\textbf{AT} = \textbf{TD}$, from which it follows that $\textbf{T}^{-1}\textbf{AT} = \textbf{D}$. Note also that since \textbf{T} is nonsingular it follows that $\textbf{T}^{-1}$ exists
\end{proof}

The ordering of the eigenvectors is not important in diagonalizing a matrix. In case the eigenvalues of a matrix are complex, the transformation matrix \textbf{T} will also be complex, and the corresponding diagonal matrix \textbf{D} will have complex entries on its diagonal.

In case the eigenvalues of a matrix are repeated, there may not exist a full set of linearly independent eigenvectors. These matrices cannot be diagonalized. However, if there exist a full set of linearly independent eigenvectors for repeated eigenvalues, then the matrix is diagonalizable. 

If a matrix is in the controllable canonical form and has distinct eigenvalues, a simple alternative to the transformation matrix for diagonalization is the following
\begin{equation}
A=\begin{bmatrix} 1 & 1 & 1 & ... & 1 \\ 
\lambda_1 & \lambda_2 & \lambda_3 & ...  & \lambda_n \\
\lambda^2_1 & \lambda^2_2 & \lambda^2_3 & ...  & \lambda^2_n \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
\lambda^{n-1}_1 & \lambda^{n-1}_2 & \lambda^{n-1}_3 & ...  & \lambda^{n-1}_n   \end{bmatrix}.
\label{vandermond matrix}
\end{equation}
The matrix in \eqref{vandermond matrix} is known as the \textbf{Vandermond matrix}. This transformation matrix is simple to construct since only the eigenvalues of the matrix are needed.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalized Eigenvectors}
\label{section: Generalized Eigenvectors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Jordan Canonical Form}
\label{section: Jordan Canonical Form}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Left Eigenvector}
\label{section: Left Eigenvector}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Eigenvalue Sensitivity}
\label{section: Eigenvalue Sensitivity}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Spectral Decomposition}
\label{section: Spectral Decomposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Functions of Matrices}
\label{section: Functions of Matrices}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% System Response %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Response}
\label{section: System Response}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Partial Differential Equations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Partial Differential Equations}
\label{section: Partial Differential Equations} 
 
%%%%%%%%%%%%%%%%%%% Exercise 4.1
\begin{exercise}
Show that the eigenvectors of a symmetric matrix are orthogonal. 
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.2
\begin{exercise}
Consider the matrix in the controllable canonical form
\begin{equation}
A=\begin{bmatrix} 0 & 1 & 0 & ... & 0 & 0 \\ 
0 & 0 & 1 & ... & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & 0 & 1 \\
-c_0 & -c_1 & -c_2 & ... & -c_{n-2} & -c_{n-1}  \end{bmatrix}.
\nonumber
\end{equation}
Show that the characteristic equation of this matrix is
\begin{equation}
\lambda^n+c_{n-1}\lambda^{n-1}+...+c_1\lambda_1=0.
\nonumber
\end{equation}
\end{exercise}

\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.3
\begin{exercise}
Consider the matric in the observable canonical form
\begin{equation}
A=\begin{bmatrix} 0 & 0 & 0 & ... & 0 & -c_0 \\ 
1 & 0 & 0 & ... & 0 & -c_1 \\
\vdots & \vdots & \vdots & \ddots & 0 & -c_{n-2} \\
0 & 0 & 0 & ... & 1 & -c_{n-1}  \end{bmatrix}.
\nonumber
\end{equation}
Show that the characteristic equation of this matrix is
\begin{equation}
\lambda^n+c_{n-1}\lambda^{n-1}+...+c_1\lambda_1=0.
\nonumber
\end{equation}
\end{exercise}

\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.4
\begin{exercise}
Consider the matrix
\begin{equation}
M=\begin{bmatrix} \textbf{A} & \textbf{0} \\ 
\textbf{-Q} & \textbf{-A'}  \end{bmatrix}
\nonumber
\end{equation}
where \textbf{A} and \textbf{Q} are square matrices. This matrix is known as the Hamiltonian matrix in control system theory. Show that the eigenvalues of \textbf{M} are symmetrically located with respect to the imaginary axis
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.5
\begin{exercise}

\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.6
\begin{exercise}

\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.7
\begin{exercise}
Given the matrix
\begin{equation}
A=\begin{bmatrix} 1 & 0 & 1 \\ 
0 & 0 & 3 \\
 0& 0 & 2  \end{bmatrix}.
\nonumber
\end{equation}
Using Cayley-Hamilton theorem, find the matrix $\textbf{B}=\sqrt{\textbf{A}}$. Also compute the eigenvalues of the matrices \textbf{A} and \textbf{B} and verify that $\lambda_\textbf{B} = \sqrt{\lambda_\textbf{A}}$.
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.8
\begin{exercise}
For the matrix given above, find the function $\textbf{B} = $sin$(\textbf{A})$. Also verify that $\lambda_\textbf{B} = $sin$(\lambda_\textbf{A})$. 
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.9
\begin{exercise}
Compute the matrix exponentual $e^\textbf{A}$ for the following matrices:
\begin{center}

$\begin{bmatrix} 2 & -1 & 0 \\ 
2 & 0 & -2 \\
 1 & -1 & 1  \end{bmatrix}$, $\begin{bmatrix} 0 & 1 & 2 \\ 
-1 & 1 & 0 \\
 0 & 0 & 2  \end{bmatrix}$, $\begin{bmatrix} 2 & -1 & 1 \\ 
0 & 2 & 0 \\
 0 & 1 & 1  \end{bmatrix}$
\end{center}
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.10
\begin{exercise}
Using the Cayley Hamilton theorem compute the inverse of the following matrices:
\begin{center}

$\begin{bmatrix} 2 & -1 & 0 \\ 
2 & 0 & -2 \\
 1 & -1 & 1  \end{bmatrix}$, $\begin{bmatrix} 0 & 1 & 2 \\ 
-1 & 1 & 0 \\
 0 & 0 & 2  \end{bmatrix}$, $\begin{bmatrix} 2 & -1 & 1 \\ 
0 & 2 & 0 \\
 0 & 1 & 1  \end{bmatrix}$
\end{center}
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.11
\begin{exercise}
Show that the eigenvalues of the matrices \textbf{A} and \textbf{A}' are identical but the eigenvectors are different. Find a relation between the two sets of eigenvectors.
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.12
\begin{exercise}
Supposed \textbf{T} is a transformation matrix that diagonalizes the matrix \textbf{A}, i.e. $\textbf{D} = \textbf{T}^{-1}\textbf{AT}$. Show that $\textbf{A}^k = \textbf{TD}^k\textbf{T}^{-1}$. Note that in this method the computation of matrix power becomes relatively simple since $\textbf{D}^k$ is easily computed. Note: The same concept holds for any other matrix function as well. 
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.13
\begin{exercise}
Find the eigenvalues and eigenvectors of the matrix $\textbf{A}^n$ where $n$ is arbitrary
\begin{center}

$\begin{bmatrix} 2 & -1 & 0 \\ 
2 & 0 & -2 \\
 1 & -1 & 1  \end{bmatrix}$, $\begin{bmatrix} 1 & 0 & -3 \\ 
0 & 1 & 2 \\
 1 & 1 & 3  \end{bmatrix}$
\end{center}
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.14
\begin{exercise}
Show that if $\lambda$ is an eigenvalue of \textbf{A} then $f(\lambda)$ is an eigenvalue of the matrix $f(\textbf{A})$, where $f$ is analytic function of its argument. Hint: Use Cayley-Hamilton theorem and the basic definition of eigenvalue problem.
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.15
\begin{exercise}
Show that for a matrix \textbf{A} with eigenvalues $\lambda_i$, $i=1,2,...,n$, show that
\begin{equation}
|\textbf{A}| = \prod_{i=1}^n \lambda_i
\nonumber
\end{equation}
\begin{equation}
Tr(\textbf{A}) = \sum_{i=1}^n \lambda_i
\nonumber
\end{equation}
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.16
\begin{exercise}
Suppose \textbf{A} is an $n \times n$ matrix whose eigenvalues are not necessarily distinct. Show that the set of eigenvectors and generalized eigenvectors form a basis for the space $\mathbb{R}^n$.
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.17
\begin{exercise}
Find the eigenvalues an eigenvectors for the following matrices:
\begin{center}

$\begin{bmatrix} 0 & \omega \\ 
 -\omega & 0  \end{bmatrix}$, $\begin{bmatrix} \sigma & \omega  \\ 
 -\omega & \sigma  \end{bmatrix}$
\end{center}
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.18
\begin{exercise}
Suppose \textbf{A} be a block diagonal matrix given by
\begin{equation}
A = \begin{bmatrix} \textbf{M}_1 & 0 & 0 \\ 
0 & \textbf{M}_2 & 0 \\ 
0 & 0 & \textbf{M}_3   \end{bmatrix}
\nonumber
\end{equation}
where the diagonal blocks are square matrices. Show that the eigenvalues of \textbf{A} are those matrices $\textbf{M}_1$, $\textbf{M}_2$, and $\textbf{M}_3$
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.19
\begin{exercise}

\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.20
\begin{exercise}
Show that the transfer function of a system is invariant under similarity transformation.
\end{exercise}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%% Exercise 4.21
\begin{exercise}
Let $\lambda$ be an eigenvalue of the matrix $\textbf{A}$. Then show that $\lambda^\phi$ is an eigenvalue of the matrix $\textbf{A}^\phi$, where $\phi$ is not necessarily an integer.
\end{exercise}
\begin{proof}

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}



